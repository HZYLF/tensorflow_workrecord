{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装pyhton3.5或者python3.6\n",
    "尽量不要装最新的3.7，据说和tensorflow有兼容问题\n",
    "\n",
    "可以使用Anaconda一步安装完成，下载指定版本的Anaconda：这里我用的是[Anaconda3-5.0.0-Windows-x86_64.exe](https://repo.anaconda.com/archive/Anaconda3-5.0.0-Windows-x86_64.exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装Tensorflow\n",
    "控制台输入\n",
    "```shell\n",
    "pip install tensorflow\n",
    "```\n",
    "(本机暂时安装不了gpu版本)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证tensorflow的安装是否准确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import tensorflow as tf\n",
    "# import pandas as pd\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# data = input_data.read_data_sets('data\\\\mnist',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载java的jdk\n",
    "pyspark需要java环境，因此需要下载java的jdk：\n",
    "- 本机下载的是[jdk-8u191-windows-x64.exe](https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-windows-x64.exe)；  \n",
    "- 下载完成后按照默认安装，并配置环境变量：\n",
    "[环境变量如何配置，点此](https://www.cnblogs.com/cnwutianhao/p/5487758.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装spark-2.2.0-bin-hadoop2.7（版本请选用此版本）\n",
    "1.去网址http://spark.apache.org/downloads.html ，重新下载[spark-2.2.0-bin-hadoop2.7.tgz](https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz) ，下载完成后，解压到了某目录，这里我是 `D:\\Extensions4Anaconda\\spark-2.2.0-bin-hadoop2.7`  \n",
    "\n",
    "2.配置环境变量：  \n",
    "*  新建SAPRK_HOME，值为`D:\\Extensions4Anaconda\\spark-2.2.0-bin-hadoop2.7`  \n",
    "*  PATH中，开头添加 `%SAPRK_HOME%\\bin`\n",
    "\n",
    "3.继续配置环境变量\n",
    "*  新建PYSPARK_DRIVEN_PYTHON，值为ipython\n",
    "*  新建PYSPARK_DRIVEN_PYTHON_OPTS，值为notebook\n",
    "*  新建PYTHONPATH，值为`%SAPRK_HOME%\\python\\lib\\py4j;%SAPRK_HOME%\\python\\lib\\pyspark`\n",
    "\n",
    "4.修改文件\n",
    "\\conf下的spark-env文件，末尾添加以下三行：\n",
    "```shell\n",
    "export PYSPARK_PYTHON=/C:/Users/zhaohang/AppData/Local/Continuum/anaconda3\n",
    "export PYSPARK_DRIVER_PYTHON=/C:/Users/zhaohang/AppData/Local/Continuum/anaconda3\n",
    "export PYSPARK_SUBMIT_ARGS='--master local[*]'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化spark参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SQLContext as sc\n",
    "def init_spark():\n",
    "    spark = SparkSession.builder.config(\"spark.debug.maxToStringFields\", \"300\").appName(\"my_first_app\").master(\"local\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "控制台会报错（如下），这里需要下载并配置\n",
    "```shell\n",
    "java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n",
    "        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)\n",
    "        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)\n",
    "        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)\n",
    "        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\n",
    "        ...\n",
    "```\n",
    "<b>下载winutils</b>，注意需要与hadoop的版本相对应，因为咱们是：spark-2.2.0-bin-hadoop2.7，所以需要下载2.7版本的winutils，去该[网址](https://github.com/steveloughran/winutils)下载，或者直接点击[这里](https://codeload.github.com/steveloughran/winutils/zip/master)下载文件  \n",
    "下载完成后，解压到某目录，我这里是`D:\\Extensions4Anaconda`，然后开始准备配置环境变量：\n",
    "\n",
    "1.新建环境变量HADOOP_HOME，值为`D:\\Extensions4Anaconda\\winutils-master\\hadoop-2.7.1`  \n",
    "2.PATH添加`%HADOOP_HOME%\\bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重启电脑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再执行以下代码\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SQLContext as sc\n",
    "def init_spark():\n",
    "    spark = SparkSession.builder.config(\"spark.debug.maxToStringFields\", \"300\").appName(\"my_first_app\").master(\"local\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "spark = init_spark()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OK，成功！</h1>\n",
    "可以打开命令行，输入\n",
    "```shell\n",
    "pyspark\n",
    "```\n",
    "如果弹出类似下面这种内容\n",
    "![avatar](./imgs/pyspark_welcome.png)  \n",
    "这说明已经成功\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
